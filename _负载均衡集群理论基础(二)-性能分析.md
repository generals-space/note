# 负载均衡集群理论基础(二)-性能分析

负载均衡是为了提高项目的访问性, 在架设集群之前首先要明白, 什么情况下需要架设负载均衡, 以及架设完成后如何衡量性能是否提升, 是否可以满足项目需求.

## 1. 明确概念

在做性能测试的时候, 很多人都用并发连接数来衡量系统的性能, 觉得系统能支撑的并发连接数越多, 系统的性能就越好; 对TPS不是非常理解, 也根本不知道它们之间的关系, 因此非常有必要进行解释.

### 1.1 并发连接数: 真正与服务器建立的(TCP)连接的数量

服务器将会存储这些连接(HTTP请求头Connection为keep-alive时), 包括其IP, 端口, 连接状态等属性, 这将对服务器资源造成消耗, 可以说是产生"压力". Nginx支持高并发就是可以同时建立多个连接并高效存储和管理, 而不至造成大量资源消耗.

### 1.2 TPS: Transaction Per Second, 每秒事务数, 是衡量系统性能的一个非常重要的指标.

客户端可以通过上述连接发起请求, 服务器则通过开辟进程/线程等手段生成响应, 这生成响应的过程便是一个事务(transaction). 在该过程中可能执行运算, 读取文件, 查询数据库等, 都将造成资源消耗. 而TPS值越大, 说明每项事务的响应时间越小, 速度越快, 这样用户体验就越好.

> PS: 在某种程度上来说, TPS有些类似于"吞吐量"这个概念.

## 2. 测试分析

我们需要了解原来服务器的性能上限值, 并且在完成负载均衡架设之后, 也需要测试集群性能是否满足预期, 是否需要调整服务器数量(资源浪费与压力过大都是不好的). 那如何判断当前架构的压力上限?( 同样配置下, 不考虑其他因素, 或者说假设所有其他可优化的点都已经做到极致.)

以Webbench测试工具为例, 测试单核CPU, 512M内存, Nginx + PHP(php-fpm) + MySQL的某Web网站.

PS: 因为不是专职的测试人员, 不需要模拟真实场景, 目的只为得到服务器的性能上限, 所以用户思考时间, 请求类型什么的不去考虑了.

### 2.1 并发数量为500时, 测试结果如下:

```
[general@localhost ~]$ webbench -c 500 -t 120 http://172.16.171.137/
Webbench - Simple Web Benchmark 1.5
Copyright (c) Radim Kolar 1997-2004, GPL Open Source Software.

Benchmarking: GET http://172.16.171.137/
500 clients, running 120 sec.

Speed=659 pages/min, 70621 bytes/sec.
Requests: 1318 susceed, 0 failed.
```

测试的同时用浏览器打开某个网页, 有延迟, 但最终还是加载出来了. CPU负载大概在13左右.

### 2.2 并发数量为2000时, 测试结果如下:

```
[general@localhost ~]$ webbench -c 2000 -t 120 http://172.16.171.137/
Webbench - Simple Web Benchmark 1.5
Copyright (c) Radim Kolar 1997-2004, GPL Open Source Software.

Benchmarking: GET http://172.16.171.137/
2000 clients, running 120 sec.

Speed=1231 pages/min, 71516 bytes/sec.
Requests: 1829 susceed, 633 failed.
```

测试的同时用浏览器打开某个网页, 出现过一次502错误, 其余两次正常. CPU负载大概在13左右.

Webbench的Speed结果可看作是TPS(不过是每分钟的), 作为生成响应的速度, 测试时-c选项的指定值则是并发连接数.

那么问题来了, 服务器的性能貌似需要"压榨", (在某个范围内)并发连接数越大, TPS越高. 但是很有可能出现请求失败的情况. 那该服务器的并发连接上限是多少? 是Requests: 0 failed或者用浏览器打开页面不出现错误时的最大并发连接数? 还是TPS最高时的连接数? 后者的值无疑会更大(我想服务器崩溃前临界并发值会更高...咳). 然而请求失败的数量肯定也会很可观.

而且在实际情况中, 我们还需要考虑用户体验. 打开一个网页需要10s, 并发再高又有什么意义呢? 从这个目的出发, 我们自然希望所有用户的请求都能得到响应, 并且在此基础上加快响应速度, 所以我们还需要限制响应时间, 超过这个时间, 就是时候着手搭建集群了(不考虑其他优化点的情况下).

## 3. 总结

集群搭建前后需要按照同一标准进行测试, 比如限制Request的failed数量小于请求总数的5%等, 这样集群相对于单独服务器的性能提升才更有说服力.